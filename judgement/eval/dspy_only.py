import dspy
from dspy.evaluate import Evaluate
from dspy.teleprompt import BootstrapFewShot
import pandas as pd

from alma.test_arena_hard import test_single_judge
from judgement.constants import BASE_LETTER_COMPARISON_CRITERIA

turbo = dspy.OpenAI(model='gpt-4o-mini', max_tokens=250)
dspy.settings.configure(lm=turbo)

df = pd.read_csv("./judgement/eval/output.csv")

dataset = []

# Later try it distinguishs which is better, that's what Alex is doing to test how good our criteria is, after we are done optimizing it.
# Maybe use it's reasoning from it's score for something
# TODO Maybe don't need to say in the prompt to optimize the criteria overtime, the DSPy is gonna do that itself.
TEMP_PROMPT = """
"** YOUR ROLE **
You are an Immigration Lawyer. Your task is to choose your preference between a rough draft of an immigration letter, generated by an LLM, and the final version edited by your co-workers.
Please output how strongly you prefer the final draft over the rough draft.
You will be given a criteria to aid you in choosing the strength of your preference.

** TASK INFORMATION **
1. You will be given a rough draft labeled "<ROUGH DRAFT>: ...", a final draft labeled "<FINAL DRAFT>: ...", and a criteria labeled "<CRITERIA>: ...".
2. Walk through each section of the letters (introduction, body, conclusion) and identify what makes the final draft more convincing. Focus on aspects such as flow, use of evidence, language choice, and structure.
3. Considering all the aspects that make the final draft better, score from 1-10 how much stronger the final draft is than the rough draft.

** OUTPUT INFORMATION **
1. <SCORE>: A 1-10 score representing how much stronger you prefer the final draft over the rough draft.
2. <REASONING>: List your reasoning behind your score.
"""

for drafts, criteria, thoughts in df.values:
    combined_prompt = f"{TEMP_PROMPT}\n{drafts}\n<CRITERIA>:{BASE_LETTER_COMPARISON_CRITERIA}\n"

    example = dspy.Example(question=combined_prompt)
    example = example.with_inputs("question")
    dataset.append(example)
    
alma_trainset, alma_devset = dataset[:7], dataset[7:]
# print(f"{type(alma_trainset[0])=}")

# Step 2: Use DSPY to optimize given the response from arena judge
# Task: Optimize the criteria prompt given to a LLM as a judge, which will score immigration letters.
# Metrics to maximize: The Arena Hard Auto score
# Few example inputs: Use the rough draft + final draft + generated criteria
# Each layer a signature: Layer 1: Input rough draft + final draft + prompt -> LLM-generated criteria.
# Layer 2: Auto arena output
# Optimizer: Compile this two step pipeline into "high-quality instructions"

class CoT(dspy.Module):
    def __init__(self):
        super().__init__()
        self.prog = dspy.ChainOfThought("question -> answer")
    
    def forward(self, question):
        return self.prog(question=question)
    
config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)

# TODO: Add call to Arena-Hard-esque LLM as a judge
def custom_metric(example: dspy.Example, pred: dspy.Prediction, trace=None) -> float:    
    rationale, answer = pred.rationale, pred.answer
    parts = answer.split("\n\n")
    score = parts[0].split("Score: ")
    
    # Extract score
    if len(score) > 1:
        score = int(parts[0].split("Score: ")[1].strip())
    
        # Extract reasoning
        # reasoning = parts[1].split("Reasoning: ")[1].strip()
    
        metric_score = score / 10
        print(f"{metric_score=}")
        return metric_score
    else:
        print("Scoring didn't work")
        # print(f"{example=}")
        print(f"{pred.answer=}")
        return 0.5

teleprompter = BootstrapFewShot(metric=custom_metric, **config)

optimized_cot = teleprompter.compile(CoT(), trainset=alma_trainset)

# Set up the evaluator, which can be used multiple times.
evaluate = Evaluate(devset=alma_devset, metric=custom_metric, num_threads=1, display_progress=True, display_table=0)

# # Evaluate our `optimized_cot` program.
evaluate(optimized_cot)

# print(turbo.inspect_history(n=1))
