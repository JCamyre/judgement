import dspy
from dspy.evaluate import Evaluate
from dspy.teleprompt import BootstrapFewShot
import pandas as pd

from alma.test_arena_hard import test_single_judge

turbo = dspy.OpenAI(model='gpt-4o-mini', max_tokens=250)
dspy.settings.configure(lm=turbo)

df = pd.read_csv("./judgement/eval/output.csv")

dataset = []

TEMP_PROMPT = """
"** YOUR ROLE **
You are an Immigration Lawyer. Your task is to review a rough draft of an immigration letter, generated by an LLM, and compare it to the final edited version by your co-workers. Based on the differences, create detailed criteria that an LLM can use to evaluate future rough drafts. 

** TASK INFORMATION **
1. You will be given a rough draft labeled "<ROUGH DRAFT>: ...", and a final draft labeled "<FINAL DRAFT>: ..."
2. Walk through each section of the letters (introduction, body, conclusion) and identify what makes the final draft more convincing. Focus on aspects such as flow, use of evidence, language choice, and structure.
3. For each aspect, determine what contributes to a strong immigration letter. Use these insights to build a scoring system from 1-5 for each category you identify.

** OUTPUT INFORMATION **
1. <CRITERIA>: Bullet points listing the detailed criteria.
"""

for drafts, criteria, thoughts in df.values:
    # TODO: Improve this prompt
    # combined_input = f"Context: {context}\nQuestion: {question}"
    prompt_and_drafts = f"{TEMP_PROMPT}\n{drafts}"

    example = dspy.Example(question=prompt_and_drafts)
    example = example.with_inputs("question")
    dataset.append(example)
    
alma_trainset, alma_devset = dataset[:7], dataset[7:]
# print(f"{type(alma_trainset[0])=}")

# Step 2: Use DSPY to optimize given the response from arena judge
# Task: Optimize the criteria prompt given to a LLM as a judge, which will score immigration letters.
# Metrics to maximize: The Arena Hard Auto score
# Few example inputs: Use the rough draft + final draft + generated criteria
# Each layer a signature: Layer 1: Input rough draft + final draft + prompt -> LLM-generated criteria.
# Layer 2: Auto arena output
# Optimizer: Compile this two step pipeline into "high-quality instructions"

class CoT(dspy.Module):
    def __init__(self):
        super().__init__()
        self.prog = dspy.ChainOfThought("question -> answer")
    
    def forward(self, question):
        return self.prog(question=question)
    
config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)

# TODO: Add call to Arena-Hard-esque LLM as a judge
def custom_metric(output: dspy.Example, reference, trace=None) -> float:
    # The question for the dspy.Example's should be: given this rough and final draft pair, generate a criteria based on it.
    print(f"{output=}")
    print(f"{reference.answer=}")
    # print(f"{trace=}")
    print("*" * 50)
    
    # TODO: All we need to pass to test_single_judge is the criteria the DSPy LLM generated.
    return test_single_judge(reference.answer)

teleprompter = BootstrapFewShot(metric=custom_metric, **config)

optimized_cot = teleprompter.compile(CoT(), trainset=alma_trainset)

# Set up the evaluator, which can be used multiple times.
evaluate = Evaluate(devset=alma_devset, metric=custom_metric, num_threads=4, display_progress=True, display_table=0)

# # Evaluate our `optimized_cot` program.
evaluate(optimized_cot)

# print(turbo.inspect_history(n=1))
